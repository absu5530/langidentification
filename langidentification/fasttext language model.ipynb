{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intelligent-yemen",
   "metadata": {},
   "source": [
    "# fastText Language Identification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "streaming-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-travel",
   "metadata": {},
   "source": [
    "## Download Tatoeba dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2021-08-11 17:17:19--  https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
      "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
      "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152301202 (145M) [application/octet-stream]\n",
      "Saving to: ‘sentences.tar.bz2’\n",
      "\n",
      "sentences.tar.bz2    15%[==>                 ]  22.52M   238KB/s    eta 4m 55s "
     ]
    }
   ],
   "source": [
    "! wget http://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
    "! bunzip2 sentences.tar.bz2\n",
    "! tar xvf sentences.tar\n",
    "! mv sentences.csv sentences.tar data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-patent",
   "metadata": {},
   "source": [
    "Create other required directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data_processed', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-burden",
   "metadata": {},
   "source": [
    "## Open dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-auction",
   "metadata": {},
   "source": [
    "There are 398 languages represented, some with very few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = pd.read_csv('data_raw/sentences.csv', sep='\\t', header=None)\n",
    "sents.columns = ['index', 'lang', 'text']\n",
    "len(sents['lang'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-joyce",
   "metadata": {},
   "source": [
    "## Get mapping of Tatoeba three-letter ISO 639-3 codes to two-letter 639-1 codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-facility",
   "metadata": {},
   "source": [
    "The Tatoeba dataset has three-letter ISO 639-3 language codes. We would like to map them to two-letter ISO 639-1 codes where available to correspond with the fastText language codes. This will require some of the codes to be mapped to their macrolanguage codes (e.g. `cmn` for Mandarin Chinese and `yue` for Yue Chinese would be mapped to `zh` for Chinese). This will cause the distinction between certain languages to be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-importance",
   "metadata": {},
   "source": [
    "### Open language to macrolanguage mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_raw/iso-639-3_Code_Tables_20210218/iso-639-3-macrolanguages.tab', 'r', encoding='utf-8-sig') as f:\n",
    "    macro_mapping = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_mapping_dict = {}\n",
    "for mapping in macro_mapping:\n",
    "    mapping_split = mapping.split('\\t')\n",
    "    macro_mapping_dict[mapping_split[1]] = mapping_split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-wales",
   "metadata": {},
   "source": [
    "### Open three-letter to two-letter mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_raw/iso-639-3_Code_Tables_20210218/iso-639-3.tab', 'r', encoding='utf-8-sig') as f:\n",
    "    three_to_two_mapping = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_to_two_mapping_dict = {}\n",
    "for mapping in three_to_two_mapping:\n",
    "    mapping_split = mapping.split('\\t')\n",
    "    three_to_two_mapping_dict[mapping_split[0]] = mapping_split[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-source",
   "metadata": {},
   "source": [
    "## Function to map language codes in Tatoeba dataset to two-letter codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-tragedy",
   "metadata": {},
   "source": [
    "Map language code to a macro code if available. Then map this code or the original to a two-letter code, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_code(lang):\n",
    "    if lang in macro_mapping_dict:\n",
    "        macro_code = macro_mapping_dict[lang]\n",
    "    else:\n",
    "        macro_code = lang\n",
    "    if macro_code in three_to_two_mapping_dict:\n",
    "        return three_to_two_mapping_dict[macro_code]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents['lang_code'] = sents['lang'].apply(lambda x: map_code(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-subdivision",
   "metadata": {},
   "source": [
    "## Filter Tatoeba data to languages with at least 100 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-burden",
   "metadata": {},
   "source": [
    "We end up with 105 languages with at least 100 examples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_value_counts = sents['lang_code'].value_counts().sort_values(ascending=False)\n",
    "lang_list = sorted_value_counts[sorted_value_counts >= 100].index.tolist()\n",
    "lang_list.remove('')\n",
    "len(lang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = sents[sents['lang_code'].isin(lang_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-rubber",
   "metadata": {},
   "source": [
    "## Get romanised South Asian language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ind = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_list = []\n",
    "texts_list = []\n",
    "for subdir, dirs, files in os.walk('data_raw/dakshina_dataset_v1.0_reduced'):\n",
    "    for file in files:\n",
    "        if 'roman.' in file:\n",
    "            with open(os.path.join(subdir, file), 'r') as f:\n",
    "                texts = f.readlines()\n",
    "            texts_list.extend([t.strip() for t in texts])\n",
    "            langs_list.extend([file.split('.')[0] + '-rom'] * len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ind['lang_code'] = langs_list\n",
    "samples_ind['text'] = texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ind.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ind['lang_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-legend",
   "metadata": {},
   "source": [
    "## Get romanised Arabic language data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-acquisition",
   "metadata": {},
   "source": [
    "Combining Egyptian Arabic, Lebanese Arabic and Tunisian Arabic (a subset with 9000 responses) datasets to get roughly 10000 responses like the South Asian data per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "egy = pd.read_csv('data_raw/Arabizi Identification/arabizi-twitter-egy.csv')\n",
    "leb = pd.read_csv('data_raw/Arabizi Identification/arabizi-twitter-leb.csv')\n",
    "tun = pd.read_csv('data_raw/tunizi_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ar = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ar['text'] = pd.concat([tun['text'].iloc[:9000], egy[egy['arabizi'] == '1']['tweet_filter'], leb[leb['arabizi'] == '1']['tweet_filter']])\n",
    "samples_ar['lang_code'] = 'ar-rom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-queens",
   "metadata": {},
   "source": [
    "## Combine all data to create augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_rom = pd.concat([samples_ind, samples_ar])\n",
    "augmented_data = pd.concat([original_data[['lang_code', 'text']], samples_rom])\n",
    "augmented_data.reset_index(inplace=True)\n",
    "augmented_data['index'] = augmented_data.index\n",
    "set(augmented_data['lang_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-limitation",
   "metadata": {},
   "source": [
    "# Define lookup table to strip out punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_table = str.maketrans(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-injury",
   "metadata": {},
   "source": [
    "## Format in fastText format and split original data into train and test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data[['index', 'lang_code', 'text']]\n",
    "original_data_list = original_data.values.tolist()\n",
    "original_data_fasttext_format = ['__label__' + data[1] + ' ' + data[2].translate(punct_table) + '\\n' for data in original_data_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-richardson",
   "metadata": {},
   "source": [
    "Language-specific punctuation is kept, e.g. in Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_fasttext_format[0:10] + original_data_fasttext_format[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_train, original_data_test = train_test_split(original_data_fasttext_format, test_size=0.2, random_state=42)\n",
    "print(len(original_data_train))\n",
    "print(len(original_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/train_original.txt', 'w') as f:    \n",
    "    f.writelines(original_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/test_original.txt', 'w') as f:    \n",
    "    f.writelines(original_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-congress",
   "metadata": {},
   "source": [
    "## Format in fastText format and split augmented data into train and test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = augmented_data[['index', 'lang_code', 'text']]\n",
    "augmented_data_list = augmented_data.values.tolist()\n",
    "augmented_data_fasttext_format = ['__label__' + data[1] + ' ' + data[2].translate(punct_table) + '\\n' for data in augmented_data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_fasttext_format[0:10] + augmented_data_fasttext_format[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_train, augmented_data_test = train_test_split(augmented_data_fasttext_format, test_size=0.2, random_state=42)\n",
    "print(len(augmented_data_train))\n",
    "print(len(augmented_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/train_augmented.txt', 'w') as f:    \n",
    "    f.writelines(augmented_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/test_augmented.txt', 'w') as f:    \n",
    "    f.writelines(augmented_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-acrobat",
   "metadata": {},
   "source": [
    "## Train models on filtered original Tatoeba data and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = fasttext.train_supervised(\"data_processed/train_original.txt\", dim=50, minn=2, maxn=4, epoch=25, loss='hs')\n",
    "model_original.quantize(input='data_processed/train_original.txt', retrain=True)\n",
    "\n",
    "original_data_test_split = [sample.split() for sample in original_data_test]\n",
    "preds_original = [model_original.predict(s[1]) for s in original_data_test_split]\n",
    "original_stats = precision_recall_fscore_support([s[0] for s in original_data_test_split], [p[0][0] for p in preds_original], average='weighted')\n",
    "print(f'Model trained on original data — Precision: {round(original_stats[0], 2)}, Recall: {round(original_stats[1], 2)}, F1 score: {round(original_stats[1], 2)}')\n",
    "\n",
    "model_original.save_model(\"models/langdetect_original.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_augmented = fasttext.train_supervised(\"data_processed/train_augmented.txt\", dim=50, minn=2, maxn=4, epoch=25, loss='hs')\n",
    "model_augmented.quantize(input='data_processed/train_augmented.txt', retrain=True)\n",
    "\n",
    "augmented_data_test_split = [sample.split() for sample in augmented_data_test]\n",
    "preds_augmented = [model_augmented.predict(s[1]) for s in augmented_data_test_split]\n",
    "augmented_stats = precision_recall_fscore_support([s[0] for s in augmented_data_test_split], [p[0][0] for p in preds_augmented], average='weighted')\n",
    "print(f'Model trained on augmented data — Precision: {round(augmented_stats[0], 2)}, Recall: {round(augmented_stats[1], 2)}, F1 score: {round(augmented_stats[1], 2)}')\n",
    "\n",
    "model_augmented.save_model(\"models/langdetect_augmented.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_original.predict('naan irukken seriya kavalaippadaathe'))\n",
    "print(model_original.predict('veettukku sendru varugiren'))\n",
    "print(model_original.predict('das habe ich nicht gesehen'))\n",
    "print(model_original.predict('ga3d tsawe al7een'))\n",
    "print(model_original.predict('konta dayir amshi le al ma7al dak fog al nil'))\n",
    "print(model_original.predict('main nahi jaa raha hoon'))\n",
    "print(model_original.predict('njan parayunna polathanne cheytha mathi'))\n",
    "print(model_original.predict('Yaarige kok, yaarige lak? Illide sambhavya sacivara patti'))\n",
    "print(model_original.predict('Bhalo achi re. Bohukaal por. Tui kemon achish?'))\n",
    "print(model_original.predict('main apni bhasha mein baat kar rahi hoon'))\n",
    "print(model_original.predict('yahan mat aaya karo'))\n",
    "print(model_original.predict('kaunsi bhaasha mein baat kar rahe ho'))\n",
    "print(model_original.predict('je ne veux pas y aller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_augmented.predict('naan irukken kavalaippadaathe'))\n",
    "print(model_augmented.predict('veettukku sendru varugiren'))\n",
    "print(model_augmented.predict('das habe ich nicht gesehen'))\n",
    "print(model_augmented.predict('ga3d tsawe al7een'))\n",
    "print(model_augmented.predict('amshi le al ma7al dak fog al nil'))\n",
    "print(model_augmented.predict('main nahi jaa raha hoon'))\n",
    "print(model_augmented.predict('njan parayunna polathanne cheytha mathi'))\n",
    "print(model_augmented.predict('Yaarige kok, yaarige lak? Illide sambhavya sacivara patti'))\n",
    "print(model_augmented.predict('Bhalo achi re. Bohukaal por. Tui kemon achish?'))\n",
    "print(model_augmented.predict('main apni bhasha mein baat kar rahi hoon'))\n",
    "print(model_augmented.predict('yahan mat aaya karo'))\n",
    "print(model_augmented.predict('kaunsi bhaasha mein baat kar rahe ho'))\n",
    "print(model_augmented.predict('je ne veux pas y aller'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-movie",
   "metadata": {},
   "source": [
    "All the examples here are coded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-savage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
