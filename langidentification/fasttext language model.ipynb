{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intelligent-yemen",
   "metadata": {},
   "source": [
    "# fastText Language Identification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "streaming-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-travel",
   "metadata": {},
   "source": [
    "## Download Tatoeba dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cloudy-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2021-08-04 16:10:02--  https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
      "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
      "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152006974 (145M) [application/octet-stream]\n",
      "Saving to: ‘sentences.tar.bz2’\n",
      "\n",
      "sentences.tar.bz2   100%[===================>] 144.96M  1.11MB/s    in 2m 12s  \n",
      "\n",
      "2021-08-04 16:12:15 (1.10 MB/s) - ‘sentences.tar.bz2’ saved [152006974/152006974]\n",
      "\n",
      "x sentences.csv\n"
     ]
    }
   ],
   "source": [
    "! wget http://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
    "! bunzip2 sentences.tar.bz2\n",
    "! tar xvf sentences.tar\n",
    "! mv sentences.csv sentences.tar data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-patent",
   "metadata": {},
   "source": [
    "Create other required directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recognized-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data_processed', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-burden",
   "metadata": {},
   "source": [
    "## Open dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-auction",
   "metadata": {},
   "source": [
    "There are 398 languages represented, some with very few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "failing-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = pd.read_csv('data_raw/sentences.csv', sep='\\t', header=None)\n",
    "sents.columns = ['index', 'lang', 'text']\n",
    "len(sents['lang'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-joyce",
   "metadata": {},
   "source": [
    "## Get mapping of Tatoeba three-letter ISO 639-3 codes to two-letter 639-1 codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-facility",
   "metadata": {},
   "source": [
    "The Tatoeba dataset has three-letter ISO 639-3 language codes. We would like to map them to two-letter ISO 639-1 codes where available to correspond with the fastText language codes. This will require some of the codes to be mapped to their macrolanguage codes (e.g. `cmn` for Mandarin Chinese and `yue` for Yue Chinese would be mapped to `zh` for Chinese). This will cause the distinction between certain languages to be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-importance",
   "metadata": {},
   "source": [
    "### Open language to macrolanguage mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advance-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_raw/iso-639-3_Code_Tables_20210218/iso-639-3-macrolanguages.tab', 'r', encoding='utf-8-sig') as f:\n",
    "    macro_mapping = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opened-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_mapping_dict = {}\n",
    "for mapping in macro_mapping:\n",
    "    mapping_split = mapping.split('\\t')\n",
    "    macro_mapping_dict[mapping_split[1]] = mapping_split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-wales",
   "metadata": {},
   "source": [
    "### Open three-letter to two-letter mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "known-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_raw/iso-639-3_Code_Tables_20210218/iso-639-3.tab', 'r', encoding='utf-8-sig') as f:\n",
    "    three_to_two_mapping = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "behavioral-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_to_two_mapping_dict = {}\n",
    "for mapping in three_to_two_mapping:\n",
    "    mapping_split = mapping.split('\\t')\n",
    "    three_to_two_mapping_dict[mapping_split[0]] = mapping_split[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-source",
   "metadata": {},
   "source": [
    "## Function to map language codes in Tatoeba dataset to two-letter codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-tragedy",
   "metadata": {},
   "source": [
    "Map language code to a macro code if available. Then map this code or the original to a two-letter code, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fresh-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_code(lang):\n",
    "    if lang in macro_mapping_dict:\n",
    "        macro_code = macro_mapping_dict[lang]\n",
    "    else:\n",
    "        macro_code = lang\n",
    "    if macro_code in three_to_two_mapping_dict:\n",
    "        return three_to_two_mapping_dict[macro_code]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consistent-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents['lang_code'] = sents['lang'].apply(lambda x: map_code(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-subdivision",
   "metadata": {},
   "source": [
    "## Filter Tatoeba data to languages with at least 100 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-burden",
   "metadata": {},
   "source": [
    "We end up with 105 languages with at least 100 examples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "missing-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_value_counts = sents['lang_code'].value_counts().sort_values(ascending=False)\n",
    "lang_list = sorted_value_counts[sorted_value_counts >= 100].index.tolist()\n",
    "lang_list.remove('')\n",
    "len(lang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "restricted-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = sents[sents['lang_code'].isin(lang_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-rubber",
   "metadata": {},
   "source": [
    "## Get romanised South Asian language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indonesian-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ind = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "preliminary-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_list = []\n",
    "texts_list = []\n",
    "for subdir, dirs, files in os.walk('data_raw/dakshina_dataset_v1.0_reduced'):\n",
    "    for file in files:\n",
    "        if 'roman.' in file:\n",
    "            with open(os.path.join(subdir, file), 'r') as f:\n",
    "                texts = f.readlines()\n",
    "            texts_list.extend([t.strip() for t in texts])\n",
    "            langs_list.extend([file.split('.')[0] + '-rom'] * len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "raised-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ind['lang_code'] = langs_list\n",
    "samples_ind['text'] = texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "swedish-viking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_code</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>jase katyamadhye nukilapan, usamadhe godva, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>Gavat kuthehe ughde gatarvevasta uplabdh nahit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>udyogache ghari devata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>Agryahun Sutaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>Dalit Premkavita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>tyanni vividh shasan padhatincha tailnik abhya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>mukhy tara 3.4 drushyapraticha piwala tara asu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>Sant Nagi hee Namdavanche putani hote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>aavhiyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mr-rom</td>\n",
       "      <td>1973 ya kalkhandat pantpradhan aslelya kitteek...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_code                                               text\n",
       "0    mr-rom  jase katyamadhye nukilapan, usamadhe godva, ne...\n",
       "1    mr-rom    Gavat kuthehe ughde gatarvevasta uplabdh nahit.\n",
       "2    mr-rom                             udyogache ghari devata\n",
       "3    mr-rom                                    Agryahun Sutaka\n",
       "4    mr-rom                                   Dalit Premkavita\n",
       "5    mr-rom  tyanni vividh shasan padhatincha tailnik abhya...\n",
       "6    mr-rom  mukhy tara 3.4 drushyapraticha piwala tara asu...\n",
       "7    mr-rom              Sant Nagi hee Namdavanche putani hote\n",
       "8    mr-rom                                           aavhiyon\n",
       "9    mr-rom  1973 ya kalkhandat pantpradhan aslelya kitteek..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_ind.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "swedish-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ta-rom    10000\n",
       "ml-rom    10000\n",
       "hi-rom    10000\n",
       "kn-rom    10000\n",
       "bn-rom    10000\n",
       "si-rom    10000\n",
       "pa-rom    10000\n",
       "te-rom    10000\n",
       "gu-rom    10000\n",
       "mr-rom    10000\n",
       "sd-rom     9999\n",
       "ur-rom     9759\n",
       "Name: lang_code, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_ind['lang_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-legend",
   "metadata": {},
   "source": [
    "## Get romanised Arabic language data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-acquisition",
   "metadata": {},
   "source": [
    "Combining Egyptian Arabic, Lebanese Arabic and Tunisian Arabic (a subset with 9000 responses) datasets to get roughly 10000 responses like the South Asian data per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "immune-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "egy = pd.read_csv('data_raw/Arabizi Identification/arabizi-twitter-egy.csv')\n",
    "leb = pd.read_csv('data_raw/Arabizi Identification/arabizi-twitter-leb.csv')\n",
    "tun = pd.read_csv('data_raw/tunizi_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "million-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ar = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bridal-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ar['text'] = pd.concat([tun['text'].iloc[:9000], egy[egy['arabizi'] == '1']['tweet_filter'], leb[leb['arabizi'] == '1']['tweet_filter']])\n",
    "samples_ar['lang_code'] = 'ar-rom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "polyphonic-evans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alah yara7me</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brabi atini najah wahed amalta fi akaber korat...</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bravo slouma walah rajel</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elboutoula ma nefhem chay</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ma7laa zinkk</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>d el zabet</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>tab law omt w gbthalk..</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>kont badawar fe el laptop la2eet awel soura la...</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>bgd ya enn fe nass amhathum msh mwguda m3ahum</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>mashy :')</td>\n",
       "      <td>ar-rom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9955 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text lang_code\n",
       "0                                          alah yara7me    ar-rom\n",
       "1     brabi atini najah wahed amalta fi akaber korat...    ar-rom\n",
       "2                              bravo slouma walah rajel    ar-rom\n",
       "3                             elboutoula ma nefhem chay    ar-rom\n",
       "4                                          ma7laa zinkk    ar-rom\n",
       "...                                                 ...       ...\n",
       "4973                                         d el zabet    ar-rom\n",
       "4982                            tab law omt w gbthalk..    ar-rom\n",
       "4983  kont badawar fe el laptop la2eet awel soura la...    ar-rom\n",
       "4984      bgd ya enn fe nass amhathum msh mwguda m3ahum    ar-rom\n",
       "4986                                          mashy :')    ar-rom\n",
       "\n",
       "[9955 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-queens",
   "metadata": {},
   "source": [
    "## Combine all data to create augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "piano-ireland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'af',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ar',\n",
       " 'ar-rom',\n",
       " 'as',\n",
       " 'az',\n",
       " 'ba',\n",
       " 'be',\n",
       " 'bg',\n",
       " 'bn',\n",
       " 'bn-rom',\n",
       " 'br',\n",
       " 'ca',\n",
       " 'ch',\n",
       " 'cs',\n",
       " 'cv',\n",
       " 'cy',\n",
       " 'da',\n",
       " 'de',\n",
       " 'el',\n",
       " 'en',\n",
       " 'eo',\n",
       " 'es',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'fa',\n",
       " 'fi',\n",
       " 'fo',\n",
       " 'fr',\n",
       " 'fy',\n",
       " 'ga',\n",
       " 'gd',\n",
       " 'gl',\n",
       " 'gn',\n",
       " 'gu',\n",
       " 'gu-rom',\n",
       " 'he',\n",
       " 'hi',\n",
       " 'hi-rom',\n",
       " 'hu',\n",
       " 'hy',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'is',\n",
       " 'it',\n",
       " 'ja',\n",
       " 'jv',\n",
       " 'ka',\n",
       " 'kk',\n",
       " 'km',\n",
       " 'kn',\n",
       " 'kn-rom',\n",
       " 'ko',\n",
       " 'ku',\n",
       " 'kw',\n",
       " 'ky',\n",
       " 'la',\n",
       " 'lb',\n",
       " 'lo',\n",
       " 'lt',\n",
       " 'lv',\n",
       " 'mi',\n",
       " 'mk',\n",
       " 'ml',\n",
       " 'ml-rom',\n",
       " 'mn',\n",
       " 'mr',\n",
       " 'mr-rom',\n",
       " 'ms',\n",
       " 'mt',\n",
       " 'my',\n",
       " 'ne',\n",
       " 'nl',\n",
       " 'no',\n",
       " 'oc',\n",
       " 'or',\n",
       " 'os',\n",
       " 'pa',\n",
       " 'pa-rom',\n",
       " 'pl',\n",
       " 'pt',\n",
       " 'qu',\n",
       " 'rn',\n",
       " 'ro',\n",
       " 'ru',\n",
       " 'sa',\n",
       " 'sd-rom',\n",
       " 'se',\n",
       " 'sh',\n",
       " 'si-rom',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'sq',\n",
       " 'sv',\n",
       " 'sw',\n",
       " 'ta',\n",
       " 'ta-rom',\n",
       " 'te',\n",
       " 'te-rom',\n",
       " 'th',\n",
       " 'ti',\n",
       " 'tk',\n",
       " 'tl',\n",
       " 'tr',\n",
       " 'tt',\n",
       " 'ug',\n",
       " 'uk',\n",
       " 'ur',\n",
       " 'ur-rom',\n",
       " 'uz',\n",
       " 'vi',\n",
       " 'vo',\n",
       " 'wo',\n",
       " 'xh',\n",
       " 'yi',\n",
       " 'zh'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_rom = pd.concat([samples_ind, samples_ar])\n",
    "augmented_data = pd.concat([original_data[['lang_code', 'text']], samples_rom])\n",
    "augmented_data.reset_index(inplace=True)\n",
    "augmented_data['index'] = augmented_data.index\n",
    "set(augmented_data['lang_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-limitation",
   "metadata": {},
   "source": [
    "# Define lookup table to strip out punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "correct-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_table = str.maketrans(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-injury",
   "metadata": {},
   "source": [
    "## Format in fastText format and split original data into train and test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "charming-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data[['index', 'lang_code', 'text']]\n",
    "original_data_list = original_data.values.tolist()\n",
    "original_data_fasttext_format = ['__label__' + data[1] + ' ' + data[2].translate(punct_table) + '\\n' for data in original_data_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-richardson",
   "metadata": {},
   "source": [
    "Language-specific punctuation is kept, e.g. in Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rapid-surfing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__zh 我們試試看！\\n',\n",
       " '__label__zh 我该去睡觉了。\\n',\n",
       " '__label__zh 你在干什麼啊？\\n',\n",
       " '__label__zh 這是什麼啊？\\n',\n",
       " '__label__zh 今天是６月１８号，也是Muiriel的生日！\\n',\n",
       " '__label__zh 生日快乐，Muiriel！\\n',\n",
       " '__label__zh Muiriel现在20岁了。\\n',\n",
       " '__label__zh 密码是Muiriel。\\n',\n",
       " '__label__zh 我很快就會回來。\\n',\n",
       " '__label__zh 我不知道。\\n',\n",
       " '__label__es Ya no lloro tanto como antes\\n',\n",
       " '__label__en Ive tried calling Tom all day but his phone is always busy\\n',\n",
       " '__label__en Tom is the fastest runner out of the three of us\\n',\n",
       " '__label__en I have a right to do what Im doing\\n',\n",
       " '__label__es Tengo el derecho a hacer lo que estoy haciendo\\n',\n",
       " '__label__en Theres things you could be doing to solve this problem\\n',\n",
       " '__label__es El terco aquí eres tú\\n',\n",
       " '__label__en Youre acting like I broke some sort of rule\\n',\n",
       " '__label__en Youre the stubborn one here\\n',\n",
       " '__label__en Dont think Im doing this to hurt you\\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data_fasttext_format[0:10] + original_data_fasttext_format[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "criminal-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6888029\n",
      "1722008\n"
     ]
    }
   ],
   "source": [
    "original_data_train, original_data_test = train_test_split(original_data_fasttext_format, test_size=0.2, random_state=42)\n",
    "print(len(original_data_train))\n",
    "print(len(original_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accessory-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/train_original.txt', 'w') as f:    \n",
    "    f.writelines(original_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "narrative-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/test_original.txt', 'w') as f:    \n",
    "    f.writelines(original_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-congress",
   "metadata": {},
   "source": [
    "## Format in fastText format and split augmented data into train and test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "secondary-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = augmented_data[['index', 'lang_code', 'text']]\n",
    "augmented_data_list = augmented_data.values.tolist()\n",
    "augmented_data_fasttext_format = ['__label__' + data[1] + ' ' + data[2].translate(punct_table) + '\\n' for data in augmented_data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "comic-curve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__zh 我們試試看！\\n',\n",
       " '__label__zh 我该去睡觉了。\\n',\n",
       " '__label__zh 你在干什麼啊？\\n',\n",
       " '__label__zh 這是什麼啊？\\n',\n",
       " '__label__zh 今天是６月１８号，也是Muiriel的生日！\\n',\n",
       " '__label__zh 生日快乐，Muiriel！\\n',\n",
       " '__label__zh Muiriel现在20岁了。\\n',\n",
       " '__label__zh 密码是Muiriel。\\n',\n",
       " '__label__zh 我很快就會回來。\\n',\n",
       " '__label__zh 我不知道。\\n',\n",
       " '__label__ar-rom selena 3mla tatto bel3rby ya gama3a\\n',\n",
       " '__label__ar-rom danty ray2a neek \\n',\n",
       " '__label__ar-rom la2 he will mat2oleesh kda\\n',\n",
       " '__label__ar-rom ybne l sa3a a5oya hyege w hyfsh5ne lw ml2hash\\n',\n",
       " '__label__ar-rom kol sa3a fe toul el seneen elly fatet 7ezent awy eny 7esertek 7afdal andam 3aleeky toul 3omry toul 3omry\\n',\n",
       " '__label__ar-rom d el zabet\\n',\n",
       " '__label__ar-rom tab law omt w gbthalk\\n',\n",
       " '__label__ar-rom kont badawar fe el laptop la2eet awel soura la2etha leeky kanet men 3and he fere7t far7et el donia el youm da konty wa7shany ad el\\n',\n",
       " '__label__ar-rom bgd ya enn fe nass amhathum msh mwguda m3ahum\\n',\n",
       " '__label__ar-rom mashy \\n']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data_fasttext_format[0:10] + augmented_data_fasttext_format[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dimensional-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6991800\n",
      "1747950\n"
     ]
    }
   ],
   "source": [
    "augmented_data_train, augmented_data_test = train_test_split(augmented_data_fasttext_format, test_size=0.2, random_state=42)\n",
    "print(len(augmented_data_train))\n",
    "print(len(augmented_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "induced-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/train_augmented.txt', 'w') as f:    \n",
    "    f.writelines(augmented_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adolescent-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processed/test_augmented.txt', 'w') as f:    \n",
    "    f.writelines(augmented_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-acrobat",
   "metadata": {},
   "source": [
    "## Train models on filtered original Tatoeba data and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cutting-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "touched-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1722008\n",
      "P@1\t0.993\n",
      "R@1\t0.993\n",
      "N\t1722008\n",
      "P@1\t0.985\n",
      "R@1\t0.985\n"
     ]
    }
   ],
   "source": [
    "model_original = fasttext.train_supervised(\"data_processed/train_original.txt\", dim=50, minn=2, maxn=4, epoch=25, loss='hs')\n",
    "print_results(*model_original.test('data_processed/test_original.txt'))\n",
    "model_original.quantize(input='data_processed/train_original.txt', retrain=True)\n",
    "print_results(*model_original.test('data_processed/test_original.txt'))\n",
    "model_original.save_model(\"models/langdetect_original.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "urban-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1747950\n",
      "P@1\t0.992\n",
      "R@1\t0.992\n",
      "N\t1747950\n",
      "P@1\t0.990\n",
      "R@1\t0.990\n"
     ]
    }
   ],
   "source": [
    "model_augmented = fasttext.train_supervised(\"data_processed/train_augmented.txt\", dim=100, minn=2, maxn=6, epoch=50, loss='hs')\n",
    "print_results(*model_augmented.test('data_processed/test_augmented.txt'))\n",
    "model_augmented.quantize(input='data_processed/train_augmented.txt', retrain=True)\n",
    "print_results(*model_augmented.test('data_processed/test_augmented.txt'))\n",
    "model_augmented.save_model(\"models/langdetect_augmented.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "christian-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__ar',), array([0.26170707]))\n",
      "(('__label__et',), array([0.29648146]))\n",
      "(('__label__de',), array([1.00003386]))\n",
      "(('__label__nl',), array([0.63696438]))\n",
      "(('__label__ar',), array([0.32579741]))\n",
      "(('__label__qu',), array([0.59138632]))\n",
      "(('__label__wo',), array([0.16203478]))\n",
      "(('__label__et',), array([0.38725895]))\n",
      "(('__label__la',), array([0.31221351]))\n",
      "(('__label__br',), array([0.60565656]))\n",
      "(('__label__et',), array([0.51756966]))\n",
      "(('__label__et',), array([0.19108413]))\n",
      "(('__label__fr',), array([0.99992788]))\n"
     ]
    }
   ],
   "source": [
    "print(model_original.predict('naan irukken seriya kavalaippadaathe'))\n",
    "print(model_original.predict('veettukku sendru varugiren'))\n",
    "print(model_original.predict('das habe ich nicht gesehen'))\n",
    "print(model_original.predict('ga3d tsawe al7een'))\n",
    "print(model_original.predict('konta dayir amshi le al ma7al dak fog al nil'))\n",
    "print(model_original.predict('main nahi jaa raha'))\n",
    "print(model_original.predict('njan parayunna polathanne cheytha mathi'))\n",
    "print(model_original.predict('Yaarige kok, yaarige lak? Illide sambhavya sacivara patti'))\n",
    "print(model_original.predict('Bhalo achi re. Bohukaal por. Tui kemon achish?'))\n",
    "print(model_original.predict('main apni bhasha mein baat kar rahi hoon'))\n",
    "print(model_original.predict('yahan mat aaya karo'))\n",
    "print(model_original.predict('kaunsi bhaasha mein baat kar rahe ho'))\n",
    "print(model_original.predict('je ne veux pas y aller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "green-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__ta-rom',), array([0.98531258]))\n",
      "(('__label__ta-rom',), array([0.44789171]))\n",
      "(('__label__de',), array([1.00003994]))\n",
      "(('__label__ar-rom',), array([0.81711674]))\n",
      "(('__label__ar-rom',), array([0.71860409]))\n",
      "(('__label__ur-rom',), array([0.94469506]))\n",
      "(('__label__ml-rom',), array([0.98487663]))\n",
      "(('__label__kn-rom',), array([0.59426987]))\n",
      "(('__label__bn-rom',), array([0.30994266]))\n",
      "(('__label__ur-rom',), array([0.7470088]))\n",
      "(('__label__ur-rom',), array([0.56993473]))\n",
      "(('__label__ur-rom',), array([0.93367803]))\n",
      "(('__label__fr',), array([0.99979973]))\n"
     ]
    }
   ],
   "source": [
    "print(model_augmented.predict('naan irukken kavalaippadaathe'))\n",
    "print(model_augmented.predict('veettukku sendru varugiren'))\n",
    "print(model_augmented.predict('das habe ich nicht gesehen'))\n",
    "print(model_augmented.predict('ga3d tsawe al7een'))\n",
    "print(model_augmented.predict('amshi le al ma7al dak fog al nil'))\n",
    "print(model_augmented.predict('main nahi jaa raha'))\n",
    "print(model_augmented.predict('njan parayunna polathanne cheytha mathi'))\n",
    "print(model_augmented.predict('Yaarige kok, yaarige lak? Illide sambhavya sacivara patti'))\n",
    "print(model_augmented.predict('Bhalo achi re. Bohukaal por. Tui kemon achish?'))\n",
    "print(model_augmented.predict('main apni bhasha mein baat kar rahi hoon'))\n",
    "print(model_augmented.predict('yahan mat aaya karo'))\n",
    "print(model_augmented.predict('kaunsi bhaasha mein baat kar rahe ho'))\n",
    "print(model_augmented.predict('je ne veux pas y aller'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-movie",
   "metadata": {},
   "source": [
    "All the examples here are coded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-savage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
